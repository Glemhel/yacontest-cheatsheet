Рассмотрим стандартную задачу проверки имплементации -- запуск unit-тестов.

Для доступа к [шаблонной задаче](https://contest.yandex.ru/admin/ng#/problem/215/2020_06_15/ndkZa0Btw4) можно написать
[@atolstikov](https://t.me/atolstikov). Будет предоставлен доступ на чтение, и можно будет клонировать задачу.

Для запуска unit-тестов на python можно использовать следующий пайплайн:
* problem-with-checker, т.е. мы запустим собственную обертку на запуск решения с компилятором (в нашем случае
  интерпретатором), а потом интерпретируем вывод этого решения в
  чекере
* В настройках соревнования для задачи оставляем единственный компилятор `Make`. Если в соревновании все задачи одного
  типа, то можно оставить единственный компилятор на соревнование, если такие задачи не все, то на вкладке Посылки,
  настраиваем отдельный компилятор для задачи.
* Если требуются дополнительные бибилиотеки (`numpy`, `pandas` и др), то можно использовать компилятор
  `yandexdataschool`. Данный компилятор иногда меняется в плане содержания библиотек, если требуется долгая поддержка
  задачи, свяжитесь в чате администраторов с командой сервиса.
* Чтобы участники видели сообщение об ошибках при вычислении итогового результата, в настройках соревнования нужно
  включить отображение вывода постпроцессора.
* Копируем в задачу файлы: `Makefile`, `run.sh`, `doit.sh`, `run_tests.py`, `check_py`, `score.py`
  * `Makefile` описывает два таргета `build` для стадии обработки присланного участником файла (в шаблонном случае файл
    переименовывается в файл `expression.py` из присланного участником, файл `doit.sh`) и стадии запуска юниттестов (для
    этого в задаче создан один фиктивный тест в наборе тестов `All tests`, запускается `score.py`)
  * `run_tests.py` берет функцию из модуля `expression.py`, запускает `unittest.TextTestRunner`, анализиует, какие тесты
    пройдены, выставляет баллы. Выводит лог тестирования в stderr: `print(string_io.getvalue(), file=sys.stderr)`.
  * `check_py` + `score.py` читают вывод `run_tests.py` и превращают его в баллы
    * скрипт должен завершаться с кодом возврата 0
    * в примере анализиутеся только последняя строка
    * в примере баллы участника от `run_tests.py` еще умножатся на множитель, который зависит от дедлайна и момента
      тестирования решение (данная резалиция будет уменьшать баллы, если решение перетестировать через несколько дней).
* Дополнительные файлы/обработки
  * Файлы для компиляции: `Makefile`, `doit.sh`, `run.sh`
  * Файлы для времени запуска: `run.sh`, `run_tests.py`
* Выставить первым файлом чекера `check_py`, а вторым -- `score.py`.

Опциональные параметры:
* открыть условие в формате html и удалить часть про ограничение на время и память, сохранить html
* можно удалить и информацию про входные и выходные файлы
* добавить в условие модель тестирования: полный или урезанный аналог `run_tests.py`
* добавить в условие заготовку файла-решения без имплементации (в html или ссылкой)

Если что-то пошло не так:
* В при тестировании не показываются баллы -- нужно выбрать тип монитора scoring
* Отправляя простой файл с реализацией, вердикт в системе `Crash`:
  * в задаче не выставлен чекер
  * чекер падает при запуске
  * в задаче нет ни одного теста
